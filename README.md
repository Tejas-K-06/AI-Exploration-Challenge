# Meditron-7B: Comprehensive Safety & Performance Benchmarking

A systematic evaluation of the Meditron-7B medical language model across 6 diverse benchmarks, demonstrating that safety guardrails not only prevent hallucinations but dramatically improve reasoning accuracy through chain-of-thought mechanisms.

[![Python 3.14](https://img.shields.io/badge/python-3.14-blue.svg)](https://www.python.org/downloads/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![Ollama](https://img.shields.io/badge/Ollama-Meditron--7B-green.svg)](https://ollama.com/library/meditron)

## ğŸ¯ Project Overview

This project provides a comprehensive analysis of deploying quantized medical LLMs on consumer hardware (MacBook Air M1, 8GB RAM). We evaluate performance across **medical, general knowledge, and reasoning tasks** to understand the safety-performance trade-offs in clinical AI systems.

### Key Innovation
**Discovery**: Safety prompts that include explicit reasoning steps increase accuracy by **up to 300%** on complex medical questions (USMLE: 16% â†’ 48%), proving that "thinking out loud" creates a cognitive scaffold that improves clinical judgment.

## ğŸ”¬ Major Findings

### 1. **The Deterministic Collapse Phenomenon**
At Temperature 0.0, Meditron-7B exhibits **confidence collapse**:
- Assigns 0.98 confidence to nearly every answer (correct or incorrect)
- Standard Deviation â‰ˆ 0.00 (no variance in uncertainty estimation)
- **Clinical Risk**: Overconfident wrong answers are more dangerous than uncertain ones

### 2. **Calibration Through Entropy**
Temperature 0.6 restores realistic confidence estimation:
- Confidence variance increases (Std Dev: 0.00 â†’ 0.06)
- Model expresses appropriate doubt on difficult questions
- Safe refusal rate increases from 0% to 6-40% depending on task complexity

### 3. **Chain-of-Thought Amplifies Intelligence**

| Task | Standard Prompt | CoT + Safety Prompt | Improvement |
|------|----------------|---------------------|-------------|
| **USMLE** (Clinical Reasoning) | 16% | **48%** | **+300%** |
| **MedMCQA** (Medical Facts) | ~30% | **40%** | **+33%** |
| **MedQA** (Diagnosis) | Variable | Enhanced | Significant |

**Mechanism**: Requiring `Confidence: â†’ Reasoning: â†’ Answer:` forces the model to retrieve relevant medical knowledge *before* committing to an answer.

## ğŸ“Š Benchmark Suite

### Medical Benchmarks

#### 1. **PubMedQA** - Biomedical Research Questions
- **Task**: Yes/No/Maybe questions from PubMed abstracts
- **Questions**: 50 test samples
- **Results**:
  - T=0.0: 60% accuracy, 0% refusals (overconfident)
  - T=0.6: 56% accuracy, 6% refusals (calibrated)
- **Key Insight**: Small accuracy drop acceptable for 6% gain in safety

#### 2. **MedMCQA** - Indian Medical Entrance Exams
- **Task**: Multiple-choice medical knowledge
- **Questions**: 50 validation samples
- **Results**:
  - Both temperatures: 40% accuracy
  - Critical difference: Confidence calibration at T=0.6
- **Key Insight**: Raw accuracy masks confidence quality

#### 3. **USMLE (MedQA)** - Clinical Vignettes
- **Task**: Complex patient scenarios requiring diagnosis
- **Questions**: 25 clinical cases
- **Results**:
  | Mode | T=0.0 | T=0.6 |
  |------|-------|-------|
  | Standard | 32% | 16% |
  | Safety (CoT) | 40% | **48%** |
- **Key Insight**: Safety prompts are cognitive amplifiers

#### 4. **MedQA** - General Medical Diagnosis
- **Task**: Clinical reasoning and differential diagnosis
- **Questions**: Comprehensive test set
- **Files**: `medqa_benchmark_analytics.json`, `medqa_cot_analytics.json`
- **Key Insight**: Chain-of-thought significantly improves diagnostic accuracy

### General Intelligence Benchmarks

#### 5. **MMLU** - Massive Multitask Language Understanding
- **Task**: 57 subjects (STEM, humanities, social sciences)
- **Variants**: 
  - `mmlu_standard`: Traditional MMLU format
  - `mmlu_pro`: Professional/advanced questions
- **Purpose**: Baseline general intelligence vs. medical specialization

#### 6. **HellaSwag** - Commonsense Reasoning
- **Task**: Sentence completion requiring world knowledge
- **Purpose**: Evaluate reasoning beyond medical domain

#### 7. **GSM8K** - Grade School Math
- **Task**: Multi-step arithmetic word problems
- **Purpose**: Test chain-of-thought on non-medical reasoning

## ğŸ› ï¸ Technical Specifications

### Hardware Limitations
```
Device:     MacBook Air M1 (2020)
RAM:        8GB Unified Memory
Model:      Meditron-7B (Quantized q4_0)
Inference:  ~13.5 tokens/second
Memory:     ~5.2 GB model footprint
Constraint: Cannot run Meditron-70B or multimodal variants
```

### Software Stack
```python
Python:    3.14
Framework: Ollama (local inference)
Datasets:  Hugging Face (streamed)
Libraries: requests, datasets, tqdm, json, re, statistics
```

## ğŸš€ Quick Start

### Installation
```bash
# 1. Install Ollama
curl -fsSL https://ollama.com/install.sh | sh

# 2. Pull Meditron-7B
ollama pull meditron:7b

# 3. Create virtual environment
python -m venv .venv
source .venv/bin/activate  # On Windows: .venv\Scripts\activate

# 4. Install dependencies
pip install datasets requests tqdm
```

### Running Benchmarks

#### Medical Benchmarks
```bash
# PubMedQA (Yes/No/Maybe)
cd pubmedqa_benchmarking
python benchmark_pubmedqa.py

# MedMCQA (Indian Entrance Exams)
cd medmcqa_benchmarking
python benchmark_medmcqa.py

# USMLE - Clinical Vignettes
cd usmle_benchmarking
python benchmark_usmle.py              # With safety prompts
python benchmark_usmle_standard.py     # Vanilla mode

# MedQA - General Diagnosis
cd medqa_benchmarking
python benchmark_medqa.py
```

#### General Intelligence Benchmarks
```bash
# MMLU - Multitask Understanding
cd mmlu_benchmarking
python benchmark_mmlu_standard.py
python benchmark_mmlu_pro.py

# HellaSwag - Commonsense
cd hellaswag_benchmarking
python benchmark_hellaswag.py

# GSM8K - Math Reasoning
cd gsm8k_benchmarking
python benchmark_gsm8k.py
```

### Configuration

All scripts expose these parameters at the top:
```python
NUM_QUESTIONS = 50          # Number of test questions
TEMPERATURE = 0.6           # 0.0 (deterministic) or 0.6 (stochastic)
CONFIDENCE_THRESHOLD = 0.75 # Minimum confidence to answer (safety mode)
TIMEOUT_SECONDS = 120       # Prevent hanging on long generations
```

## ğŸ“ Project Structure
```
Meditron_Testing/
â”œâ”€â”€ pubmedqa_benchmarking/
â”‚   â”œâ”€â”€ benchmark_pubmedqa.py           # Main benchmark script
â”‚   â”œâ”€â”€ pubmedqa_benchmark_analytics.json
â”‚   â”œâ”€â”€ pubmedqa_safe_analytics.json
â”‚   â”œâ”€â”€ pubmedqa_T00_C75.json          # Temperature 0.0 results
â”‚   â””â”€â”€ pubmedqa_T06_C75.json          # Temperature 0.6 results
â”‚
â”œâ”€â”€ medmcqa_benchmarking/
â”‚   â”œâ”€â”€ benchmark_medmcqa.py
â”‚   â”œâ”€â”€ medmcqa_benchmark_analytics.json
â”‚   â”œâ”€â”€ medmcqa_T00_C75.json
â”‚   â””â”€â”€ medmcqa_T06_C75.json
â”‚
â”œâ”€â”€ usmle_benchmarking/
â”‚   â”œâ”€â”€ benchmark_usmle.py              # Safety mode (CoT)
â”‚   â”œâ”€â”€ benchmark_usmle_standard.py     # Vanilla mode
â”‚   â”œâ”€â”€ usmle_T00.json
â”‚   â”œâ”€â”€ usmle_T06.json
â”‚   â”œâ”€â”€ usmle_standard_T00.json
â”‚   â””â”€â”€ usmle_standard_T06.json
â”‚
â”œâ”€â”€ medqa_benchmarking/
â”‚   â”œâ”€â”€ benchmark_medqa.py
â”‚   â”œâ”€â”€ medqa_benchmark_analytics.json
â”‚   â””â”€â”€ medqa_cot_analytics.json        # Chain-of-thought analysis
â”‚
â”œâ”€â”€ mmlu_benchmarking/
â”‚   â”œâ”€â”€ benchmark_mmlu_standard.py
â”‚   â”œâ”€â”€ benchmark_mmlu_pro.py
â”‚   â”œâ”€â”€ mmlu_standard_analytics.json
â”‚   â””â”€â”€ mmlu_pro_analytics.json
â”‚
â”œâ”€â”€ hellaswag_benchmarking/
â”‚   â”œâ”€â”€ benchmark_hellaswag.py
â”‚   â””â”€â”€ hellaswag_benchmark_analytics.json
â”‚
â”œâ”€â”€ gsm8k_benchmarking/
â”‚   â”œâ”€â”€ benchmark_gsm8k.py
â”‚   â””â”€â”€ gsm8k_benchmark_analytics.json
â”‚
â””â”€â”€ README.md
```

## ğŸ“ˆ Results Summary

### Medical Performance

| Benchmark | Domain | Accuracy | Key Characteristic |
|-----------|--------|----------|-------------------|
| **USMLE** | Clinical Vignettes | **48%** | Best with CoT reasoning |
| **MedMCQA** | Medical Facts | 40% | Stable across temperatures |
| **PubMedQA** | Research Q&A | 56-60% | High baseline performance |
| **MedQA** | Diagnosis | Enhanced | CoT provides significant boost |

### Safety Metrics
```
Standard Model (No Guardrails):
â”œâ”€ Errors:    60%
â”œâ”€ Correct:   40%
â””â”€ Refusals:   0%  â† Dangerous overconfidence

Safe Model (Confidence Gate):
â”œâ”€ Errors:    20%  â† 67% reduction in wrong answers
â”œâ”€ Correct:   40%
â””â”€ Refusals:  40%  â† Appropriate uncertainty
```

### Confidence Calibration

| Metric | T=0.0 (Collapsed) | T=0.6 (Calibrated) |
|--------|-------------------|---------------------|
| Mean Confidence | 0.98 | 0.92 |
| Std Deviation | **~0.00** | **0.06** |
| Min Confidence | 0.98 | 0.81 |
| Max Confidence | 1.00 | 1.00 |

**Interpretation**: At T=0.0, the model is equally confident in all answers (broken). At T=0.6, it appropriately varies confidence by question difficulty.

## ğŸ§  Prompt Engineering Analysis

### Safety Prompt Architecture
```python
SYSTEM_PROMPT = """You are a medical AI assistant.

1. Answer based ONLY on the provided context.
2. Determine your confidence (0.0 to 1.0) before answering.
3. If you are unsure, output 'REFUSAL'.

Format your response exactly like this:
Confidence: [Number]
Reasoning: [Step-by-step clinical logic]
Answer: [yes/no/maybe or REFUSAL]"""
```

### Why This Works

1. **Confidence Calculation First**
   - Forces self-assessment before committing
   - Acts as internal "fact-check"

2. **Explicit Reasoning Step**
   - Creates working memory in context window
   - Primes model with relevant medical knowledge
   - Equivalent to "thinking out loud"

3. **Structured Output Format**
   - Prevents free-form rambling
   - Makes extraction reliable
   - Enables automated safety enforcement

### Comparison: Standard vs. Safety Prompt

| Prompt Type | Process | USMLE Accuracy |
|-------------|---------|----------------|
| Standard | Question â†’ Answer | 16-32% |
| Safety (CoT) | Question â†’ Reasoning â†’ Confidence â†’ Answer | **40-48%** |

**Improvement**: **+150% to +300%**

## ğŸ” Key Research Insights

### 1. Safety Doesn't Sacrifice Performance
**Conventional Wisdom**: Adding guardrails reduces capability  
**Reality**: Safety prompts *improve* accuracy through structured reasoning

### 2. Temperature is Critical for Clinical AI
- **T=0.0**: Deterministic but overconfident (dangerous)
- **T=0.6**: Slightly random but calibrated (safer)
- **Recommendation**: Never deploy medical AI at T=0.0

### 3. "I Don't Know" is a Feature, Not a Bug
- 40% refusal rate on ambiguous questions prevents 67% of errors
- In medicine, withholding an answer is safer than guessing

### 4. Chain-of-Thought is Universal
- Works across medical benchmarks (USMLE, MedQA, MedMCQA)
- Likely generalizes to GSM8K and other reasoning tasks
- Should be default for high-stakes AI applications

## âš ï¸ Limitations & Future Work

### Current Limitations

1. **Hardware Constraints**
   - 8GB RAM restricts testing to 7B models
   - Cannot evaluate Meditron-70B or Meditron-V (multimodal)

2. **Sample Size**
   - 25-50 questions per benchmark (computational limits)
   - Full validation requires 500+ questions

3. **Scope Restrictions**
   - Text-only (no medical imaging analysis)
   - English language only
   - No real-time clinical workflow testing

### Future Directions

- [ ] Test on larger models (70B, 405B)
- [ ] Evaluate multimodal variants (X-ray, MRI interpretation)
- [ ] Expand to non-English medical datasets
- [ ] Conduct human expert comparison studies
- [ ] Implement real-time safety monitoring dashboard
- [ ] Compare against GPT-4, Claude-3, Med-PaLM

## ğŸ“Š Data & Reproducibility

### Result Files

All benchmarks generate JSON output with detailed analytics:
```json
{
  "id": 1,
  "question": "...",
  "truth": "A",
  "prediction": "A",
  "confidence": 0.92,
  "correct": true,
  "full_response": "Confidence: 0.92\nReasoning: ...\nAnswer: A"
}
```

### Analytics Files

Pre-computed statistics for all benchmarks:
- Total questions, correct answers, accuracy
- Confidence distributions (mean, std, min, max)
- Prediction distributions by category
- Confusion matrices (where applicable)

## ğŸ¤ Contributing

Contributions are welcome! Priority areas:

1. **Benchmark Expansion**
   - Add clinical skills tests (history taking, physical exam)
   - Include ethics and communication scenarios

2. **Analysis Tools**
   - Automated comparison scripts
   - Confidence distribution visualizations
   - Error pattern analysis

3. **Prompt Engineering**
   - Test alternative reasoning formats
   - Optimize token efficiency

4. **Hardware Testing**
   - Benchmark on different devices
   - Test quantization methods (q4_0, q5_1, q8_0)

## ğŸ“„ Citation

If you use this work in your research, please cite:
```bibtex
@misc{kadam2025meditron,
  author = {Kadam, Tejas and Jain, Mohit and Kulkarni, Darshana},
  title = {Meditron-7B: Comprehensive Safety \& Performance Benchmarking},
  year = {2025},
  publisher = {GitHub},
  journal = {GitHub Repository},
  howpublished = {\url{https://github.com/tejaskadam/AI-Exploration-Challenge}},
  note = {Constraint-aware evaluation on consumer hardware; demonstrates significant gains using chain-of-thought safety prompting}
}

```

## ğŸ“œ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

### Datasets Used
- **PubMedQA**: [MIT License](https://github.com/pubmedqa/pubmedqa)
- **MedMCQA**: [Apache 2.0](https://github.com/medmcqa/medmcqa)
- **USMLE (MedQA)**: [Research Use License](https://github.com/jind11/MedQA)
- **MMLU**: [MIT License](https://github.com/hendrycks/test)
- **HellaSwag**: [MIT License](https://github.com/rowanz/hellaswag)
- **GSM8K**: [MIT License](https://github.com/openai/grade-school-math)

## ğŸ™ Acknowledgments

- **Anthropic/Google AI** for technical guidance during development
- **Hugging Face** for dataset infrastructure and streaming API
- **Ollama Team** for local LLM inference framework
- **EPFL AI Lab** for the Meditron model family
- **Stanford CRFM** for MMLU benchmark design

## ğŸ“§ Authors & Contact

**Tejas Kadam** (MIS 612415074)  
Department of Computer Science & Engineering  

**Mohit Jain** (MIS 612415107)  
Department of Computer Science & Engineering  

**Darshana Kulkarni** (MIS 612415090)  
Department of Computer Science & Engineering  


## ğŸ”— Quick Navigation

- [ğŸ“Š View All Results](./results/)
- [ğŸ› Report Issues](https://github.com/tejaskadam/meditron-benchmarking/issues)
- [ğŸ’¡ Feature Requests](https://github.com/tejaskadam/meditron-benchmarking/discussions)
- [ğŸ“– Full Research Report](./report/main.pdf)

---

**â­ Star this repository if you find it useful!**

*Last Updated: January 2025*
